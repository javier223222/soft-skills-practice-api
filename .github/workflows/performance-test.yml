name: ⚡ Performance Testing

on:
  schedule:
    # Run performance tests every Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
      users:
        description: 'Number of concurrent users'
        required: false
        default: '10'

jobs:
  performance-test:
    name: ⚡ Load Testing
    runs-on: ubuntu-latest

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🚀 Start services
      run: |
        cp .env.example .env
        docker-compose up -d --build
        
    - name: ⏳ Wait for services
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:8000/health/; do sleep 2; done'

    - name: 📚 Populate test data
      run: |
        docker-compose exec -T api python scripts/populate_data.py

    - name: 🐍 Set up Python for testing
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: 📦 Install testing tools
      run: |
        pip install locust requests

    - name: 📝 Create Locust test file
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import random
        import json

        class SoftSkillsUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Get available soft skills and scenarios
                response = self.client.get("/soft-skills/")
                if response.status_code == 200:
                    self.skills = response.json()
                    if self.skills:
                        # Get scenarios for first skill
                        skill_id = self.skills[0]["id"]
                        scenarios_response = self.client.get(f"/soft-skills/{skill_id}/scenarios")
                        if scenarios_response.status_code == 200:
                            self.scenarios = scenarios_response.json()
                        else:
                            self.scenarios = []
                    else:
                        self.scenarios = []
                else:
                    self.skills = []
                    self.scenarios = []

            @task(3)
            def view_soft_skills(self):
                """Test getting soft skills list"""
                self.client.get("/soft-skills/")

            @task(2)
            def view_scenarios(self):
                """Test getting scenarios for a skill"""
                if self.skills:
                    skill_id = random.choice(self.skills)["id"]
                    self.client.get(f"/soft-skills/{skill_id}/scenarios")

            @task(1)
            def health_check(self):
                """Test health endpoint"""
                self.client.get("/health/")

            @task(1)
            def complete_practice_flow(self):
                """Test complete practice session flow"""
                if not self.skills or not self.scenarios:
                    return
                
                # Start practice session
                skill = random.choice(self.skills)
                scenario = random.choice(self.scenarios)
                
                start_data = {
                    "user_id": f"load_test_user_{random.randint(1, 1000)}",
                    "soft_skill_id": skill["id"],
                    "scenario_id": scenario["id"]
                }
                
                start_response = self.client.post("/practice/start", json=start_data)
                
                if start_response.status_code == 200:
                    session_data = start_response.json()
                    session_id = session_data["session_id"]
                    
                    # Submit practice
                    submit_data = {
                        "session_id": session_id,
                        "user_input": "This is a load testing response. I would handle this situation professionally and with empathy, ensuring clear communication and understanding all perspectives involved.",
                        "duration_seconds": random.randint(120, 600)
                    }
                    
                    self.client.post("/practice/submit", json=submit_data)

            @task(1)
            def view_user_progress(self):
                """Test user progress endpoint"""
                user_id = f"load_test_user_{random.randint(1, 100)}"
                self.client.get(f"/progress/{user_id}")
        EOF

    - name: ⚡ Run performance tests
      run: |
        DURATION=${{ github.event.inputs.duration || '300' }}
        USERS=${{ github.event.inputs.users || '10' }}
        
        echo "🚀 Starting load test with $USERS users for ${DURATION}s"
        
        locust -f locustfile.py --headless \
               --users $USERS \
               --spawn-rate 2 \
               --run-time ${DURATION}s \
               --host http://localhost:8000 \
               --html performance_report.html \
               --csv performance_data

    - name: 📊 Upload performance report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-report
        path: |
          performance_report.html
          performance_data_*.csv

    - name: 📈 Analyze results
      run: |
        echo "📊 Performance Test Results Summary"
        echo "=================================="
        
        if [ -f performance_data_stats.csv ]; then
          echo "📈 Request Statistics:"
          cat performance_data_stats.csv
          echo ""
        fi
        
        if [ -f performance_data_failures.csv ]; then
          echo "❌ Failures:"
          cat performance_data_failures.csv
          echo ""
        fi
        
        # Check for performance issues
        if [ -f performance_data_stats.csv ]; then
          # Extract average response time for main endpoints
          avg_response_time=$(tail -n +2 performance_data_stats.csv | awk -F',' '{sum+=$9; count++} END {if(count>0) print sum/count; else print 0}')
          
          echo "Average response time: ${avg_response_time}ms"
          
          # Fail if average response time is too high (>2000ms)
          if (( $(echo "$avg_response_time > 2000" | bc -l) )); then
            echo "❌ Performance test failed: Average response time too high"
            exit 1
          else
            echo "✅ Performance test passed"
          fi
        fi

    - name: 🔽 Cleanup
      if: always()
      run: |
        docker-compose down -v
